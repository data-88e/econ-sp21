{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<table style=\"width: 100%;\" id=\"nb-header>\">\n",
    "        <tr style=\"background-color: transparent;\"><td>\n",
    "            <img src=\"https://d8a-88.github.io/assets/images/blue_text.png\" width=\"250px\" style=\"margin-left: 0;\" />\n",
    "        </td><td>\n",
    "            <p style=\"text-align: right; font-size: 10pt;\"><strong>Economic Models</strong>, Fall 2020<br>\n",
    "                Dr. Eric Van Dusen<br>\n",
    "            Notebook by Andrei Caprau<br>\n",
    "            </p></td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 11: Econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## What is Econometrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "One way to think about Econometrics is that it is the closest economists can usually get to doing experiments in the same way experiments are done in medicine and other fields involving people. The idealized experiment is one where people are randomly assigned to either the treatment or control group such that the participants *and* the researchers don't know which group each person is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "By randomly assigning a large sample of participants to two different groups, we can be fairly confident that on average the two groups are identical in their attributes, apart from the treatment that only one group receives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "By having a double-blind experiment, we can be confident that the participants and researchers aren't conciously or subconciously (placebo effect) affecting the results of the treatment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Because of this setup, we can look at the difference in outcomes of the two groups and be fairly confident that any differences are due to the treatment and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "But it's usually impossible or unethical to perform an ideal experiment in economics. Imagine trying to examine the effect of years of schooling on future earnings. Can you force people into a treatment group with more years of schooling and a control group with less? Is it sufficient to just collect a sample of people and compare the earnings of people with high schooling to those with low schooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How can we answer the above question and others like it? Econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Econometrics is a vast field and today we will just focus on the basics of something called regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data 8 Review of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import patches\n",
    "from datascience import *\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suppose we have some data that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![title](figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Recall from Data 8 that $x$ and $y$ above have some **correlation coefficient** $r$, which is a measure of the strength of the linear relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It looks like there is some positive linear association between $x$ and $y$ such that larger values of $x$ correspond to larger values of $y$. We therefore expect $r$ to be some positive number between 0 and 1, but not exactly 0 or exactly 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First, let's convert the data to standard units. Below we construct a function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def standard_units(array):\n",
    "    return (array - np.mean(array)) / np.std(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Hide this.\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0, 10, 100)\n",
    "noise = np.random.randn(100) * 4\n",
    "y = 1.5 * x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard = standard_units(x)\n",
    "y_standard = standard_units(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_standard, y_standard)\n",
    "plt.xlabel('$x$_standard')\n",
    "plt.ylabel('$y$_standard');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot looks the same as before, except now the axes are scaled such that we measure $x$ and $y$ in standard units. Now recall that $r$ is calculated as the average of the product of two variables, when the variables are measured in standard units. Below we define a function that calculates $r$, assuming that the inputs have already been converted to standard units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(array1, array2):\n",
    "    return np.mean(array1 * array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What is the correlation between these two variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(x_standard, y_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Data 8 that we use $r$ to form a line called the *regression line*, which makes predictions for $y$ given some $x$. Our prediction for $y$ in standard units is $r \\cdot x$. If we want to fit this regression line in the original units, recall that the slope of this line is given by\n",
    "\n",
    "$$\n",
    "\\text{slope} = r \\cdot \\dfrac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x}\n",
    "$$\n",
    "\n",
    "and the intercept is given by\n",
    "\n",
    "$$\n",
    "\\text{intercept} = \\hat{\\mu}_y - \\text{slope} \\cdot \\hat{\\mu}_x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = correlation(x_standard, y_standard)\n",
    "slope = r * np.std(y) / np.std(x)\n",
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the slope we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Slope: ', slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the data above, what I did is I started with some range of $x$ values, and generated $y$ as a linear function of $x$ with some random noise added in. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.uniform(0, 10, 100)\n",
    "noise = np.random.randn(100) * 4\n",
    "y = 1.5 * x + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how I defined $y$:\n",
    "\n",
    "$$\n",
    "y = 1.5 \\cdot x + u\n",
    "$$\n",
    "\n",
    "where $u$ is some random noise whose average is 0. So, while there is some randomness to the data, on average the \"true\" slope of the relationship is 1.5. Yet we predicted it to be roughly 1.3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This highlights the following fact: Suppose we have some random data that we believe has a linear relationship. The least-squares slope we generate from the data is an *estimate* of the \"true\" slope of that data. Because of this, the estimated slope is a random variable that depends on the data we happen to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight this fact, let's repeat the procedure above but with a different [random seed](https://en.wikipedia.org/wiki/Random_seed), in order to get data with the same underlying relationship but different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(189)\n",
    "x = np.random.uniform(0, 10, 100)\n",
    "noise = np.random.randn(100) * 4\n",
    "y = 1.5 * x + noise\n",
    "\n",
    "r = correlation(x_standard, y_standard)\n",
    "slope = r * np.std(y) / np.std(x)\n",
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');\n",
    "print('Slope: ', slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the estimated slope is roughly 1.6, even though the underlying data was still generated using a slope of 1.5. This is a very important concept that we will revisit soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, however, that correlation in data *does not* imply causation. In this example we know the true causal relationship between $x$ and $y$ because we defined it ourselves. However, when using real data you do not see the \"true\" relation and thus cannot conclude causality from correlation. It could simply be that both your variables depend on an unseen third variable and have no causal effect on one another. Or even worse, while unlikely it could be the case that slight linear trends in two variables is a complete coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Table().with_columns(\n",
    "    'x', make_array(0,  1,  2,  3,  4),\n",
    "    'y', make_array(1, .5, -1,  2, -3))\n",
    "\n",
    "def rmse(target, pred):\n",
    "    return np.sqrt(mean_squared_error(target, pred))\n",
    "\n",
    "def plot_line_and_errors(slope, intercept):\n",
    "    print(\"RMSE:\", rmse(slope * d.column('x') + intercept, d.column('y')))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    points = make_array(-2, 7)\n",
    "    p = plt.plot(points, slope*points + intercept, color='orange', label='Proposed line')\n",
    "    ax = p[0].axes\n",
    "    \n",
    "    predicted_ys = slope*d.column('x') + intercept\n",
    "    diffs = predicted_ys - d.column('y')\n",
    "    for i in np.arange(d.num_rows):\n",
    "        x = d.column('x').item(i)\n",
    "        y = d.column('y').item(i)\n",
    "        diff = diffs.item(i)\n",
    "        \n",
    "        if diff > 0:\n",
    "            bottom_left_x = x\n",
    "            bottom_left_y = y\n",
    "        else:\n",
    "            bottom_left_x = x + diff\n",
    "            bottom_left_y = y + diff\n",
    "        \n",
    "        ax.add_patch(patches.Rectangle(make_array(bottom_left_x, bottom_left_y), abs(diff), abs(diff), color='red', alpha=.3, label=('Squared error' if i == 0 else None)))\n",
    "        plt.plot(make_array(x, x), make_array(y, y + diff), color='red', alpha=.6, label=('Error' if i == 0 else None))\n",
    "    \n",
    "    plt.scatter(d.column('x'), d.column('y'), color='blue', label='Points')\n",
    "    \n",
    "    plt.xlim(-4, 8)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.8, .8))\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_line_and_errors, slope=widgets.FloatSlider(min=-4, max=4, step=.1), intercept=widgets.FloatSlider(min=-4, max=4, step=.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Back to Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from above that if we have some (well-behaved) data, we can fit a least-squares line onto that data. That line can have two purposes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Of particular interest to data scientists is the line's ability to predict values of $y$ for new values of $x$ that we didn't see before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Of particular interest to economists is the line's ability to estimate the \"true\" underlying slope of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why regression is such a powerful tool and forms the backbone of econometrics. If we believe that our data satisfy certain assumptions (which we won't explore too much this lecture), then we can use the slope of the regression line to estimate the \"true\" relation between the variables in question and learn more about the world we live in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In econometrics, we usually write the \"true\" underlying linear relationship as follows:\n",
    "\n",
    "$$\n",
    "y = \\alpha + \\beta \\cdot x + \\varepsilon\n",
    "$$\n",
    "\n",
    "where $y$ and $x$ are values for any arbitrary point, $\\alpha$ is the intercept, $\\beta$ is the slope, and $\\varepsilon$ is some noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is entirely analogous to the code from earlier that determined the true linear relationship between the data:\n",
    "\n",
    "```python\n",
    "y = 1.5 * x + noise\n",
    "```\n",
    "\n",
    "Here, $\\beta = 1.5$, $\\alpha = 0$, and $\\varepsilon = \\text{noise}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit a regression line onto the data, we express the line as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\alpha} + \\hat{\\beta} \\cdot x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we put hats over the slope and intercept terms because they are *estimates* of the true slope and intercept terms. Similarly, we put a hat over $y$ because this is the $y$ value that the regression line predicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the noise term $\\varepsilon$ does not appear in the expression for the regression line. This is because the noise term is a random variable that has no relation with $x$, and is thus impossible to predict from the data. Furthermore, the noise term has a mean value of 0, so on average we actually don't expect the noise term to have any impact on the underlying trends of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Data 8 demonstration above, we forced these conditions to be true. However, with real data these are assumptions that we have to make, and is something that econometricians spend a lot of time thinking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Years of Schooling and Earnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a case where we want to study how years of schooling relate to a person's earnings. This should be of particular interest to college students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import a dataset that has the hourly wage, years of schooling, and other information on thousands of people sampled in the March 2012 Current Population Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps = Table.read_table('cps.csv')\n",
    "cps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to consider a person's wage and years of schooling. But first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Detour on Wage and Similar Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wage is an interesting variable because it is something that tends to vary by a proportion rather than an absolute amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight this fact, let's actually consider how the variable GDP behaves. GDP tends to grow by a certain percent each year; no one is particularly interested in how *much* GDP changes from one year to the next, or from one country to another, but rather by *what percent* it changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from the finance week, this behaves similarly to compound interest. If you were to plot GDP over time for a country, it might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDPs = []\n",
    "GDPs.append(100)\n",
    "\n",
    "for _ in range(99):\n",
    "    GDPs.append(GDPs[-1] * 1.05)\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(100), GDPs)\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('GDP')\n",
    "plt.title('GDP Over Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting value for GDP is 100, and GDP grows by 5 percent each year. Look at how how much GDP grows in the later years!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this phenomenon is impressive, it is misleading. At surface level, it seems to imply that something different happened at around year 50 or so that caused GDP to increase considerably in subsequent years. We know that this isn't true, and that this is just a consequence of exponential growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To counter this effect, for variables that tend to vary from one observation to the next by proportions rather than absolute amounts, we take the natural log of these variables. Let's do that for GDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_GDPs = np.log(GDPs)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(100), ln_GDPs)\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('GDP')\n",
    "plt.title('GDP Over Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now uncovered a linear relationship between years and GDP! You can interpret the slope of this line as the approximate *percent change* in GDP for an increase in one year. To verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Slope between years 0 and 1: ', ln_GDPs[1] - ln_GDPs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that wage is another one of these variables, and so when we do studies on wage we usually take the natural log of wage instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot log wage and years of schooling for the CPS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ = cps.column('educ')\n",
    "logwage = cps.column('logwage')\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(educ, logwage)\n",
    "plt.xlabel('Years of Education')\n",
    "plt.ylabel('Log Wage')\n",
    "plt.title('Log Wage vs. Years of Education');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a least-squares regression line onto this data. First, we'll do it manually in the Data 8 style above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educ_standard = standard_units(educ)\n",
    "logwage_standard = standard_units(logwage)\n",
    "\n",
    "r = correlation(logwage_standard, educ_standard)\n",
    "slope = r * np.std(logwage) / np.std(educ)\n",
    "intercept = np.mean(logwage) - slope * np.mean(educ)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(educ, logwage)\n",
    "plt.plot(np.linspace(0, 20), slope * np.linspace(0, 20) + intercept, color='tab:orange')\n",
    "plt.xlabel('Years of Education')\n",
    "plt.ylabel('Log Wage')\n",
    "plt.title('Log Wage vs. Years of Education');\n",
    "print('Slope: ', slope)\n",
    "print('Intercept: ', intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there are statistical packages that actually do all this work for you. From now on we'll use this instead. Below we verify that we get the same results using the package `statsmodels`, which is the main statistics package in Python and which we have imported as `sm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do single variable regression in Python, we use the `statsmodels` package, which has a very simple flow:\n",
    "\n",
    "```python\n",
    "X = tbl.column(feature)                     # Separate explanatory variable\n",
    "y = tbl.column(target)                      # Separate outcome variable\n",
    "model = sm.OLS(y, sm.add_constant(X))       # Initialize the OLS regression model\n",
    "result = model.fit()                        # Fit the regression model and save it to a variable\n",
    "result.summary()                            # Display a summary of results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order for `statsmodels` to give us an intercept term, we must add a column of all 1's to the data. This is typical in econometrics and you should essentially always have this constant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.add_constant(educ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(logwage, sm.add_constant(educ)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, the numbers are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the very simple and straight-forward model above, it seems that we estimate a slope of roughly 0.1, meaning we might expect that a one-year increase in schooling is associated with a 10 percent increase in wage, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that we have a non-zero intercept term. We should be careful how we interpret this term; from a strictly mathematical point of view, the intercept represents the expected value of $y$ (in this case log wage) when $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in economics sometimes it makes no sense for $x$ to be 0, and so we cannot use the above interpretation. We won't go into detail this lecture, but regardless of whether the intercept is interpretable, we almost always want to include it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty in $\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that the slope we estimate from regression is exactly that: an estimate of the \"true\" underlying slope. Because of this, the estimate $\\hat{\\beta}_1$ is a random variable that depends on the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume there is some true linear relation between log wage and years of schooling,\n",
    "\n",
    "$$\n",
    "\\text{logwage} = \\alpha + \\beta \\cdot \\text{years of schooling} + \\varepsilon\n",
    "$$\n",
    "\n",
    "and we try to estimate $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our data are \"well-behaved\", then even though there is uncertainty in our estimate $\\hat{\\beta}$, on average $\\hat{\\beta}$ will be $\\beta$; that is to say that the expectation of $\\hat{\\beta}$ is $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if our data are \"well-behaved\", then $\\hat{\\beta}$ has some normal distribution with mean $\\beta$. We won't worry too much about what assumptions need to be satisfied to make the data \"well-behaved\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each person as an observation of these variables, and using a sample of people we can estimate the relationship between the two variables. However, due to the noise term and the fact that we only have a finite sample of people, the true relationship is always hidden from us, and we can only hope to get better estimates by designing better experiments and sampling more people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get an idea of how \"certain\" we can be of our estimate $\\hat{\\beta}$. We'll do this in classic Data 8 style: bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our existing sample data, we'll create new samples by bootstrapping from the existing data. Then, for each sample, we'll fit a line, and keep the slope of that line in a list with all of the other slopes. Then, we'll find the standard deviation of that list of slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = make_array()\n",
    "educ_logwage = cps.select(\"educ\", \"logwage\")\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in np.arange(200):\n",
    "    educ_logwage_sample = educ_logwage.sample()\n",
    "    y = educ_logwage_sample.column(\"logwage\")\n",
    "    X = educ_logwage_sample.column(\"educ\")\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    slopes  = np.append(model.params[1], slopes)\n",
    "Table().with_columns(\"Slopes\", slopes).hist()    \n",
    "print('Standard dev. of bootstrapped slopes: ', np.std(slopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an estimate of the standard deviation of $\\hat{\\beta}$. This is called the *standard error* of $\\hat{\\beta}$. `statsmodels` actually already does this for us. Look at the column called \"std err\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(logwage, sm.add_constant(educ)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary table, `statsmodels` rounds numbers, so let's see an expanded form of the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SE of beta 1: ', model.bse[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bootstrapped approximation standard error of 0.00159 is pretty close to the true standard error of 0.00144. `statsmodels` actually uses a precise mathematical formula for finding the standard error whereas we tried to find this value through simulation, but the idea behind the standard error is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a standard error, we can now form a 95% confidence interval and perform a test of significance to see if $\\hat{\\beta}$ is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our resampled slopes\n",
    "lower_bound = percentile(2.5, slopes)\n",
    "upper_bound = percentile(97.5, slopes)\n",
    "print('95% confidence interval: [{}, {}]'.format(lower_bound, upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the given standard deviation in sample slopes from statsmodels\n",
    "lower_bound = model.params[1] - 1.96 * model.bse[1]\n",
    "upper_bound = model.params[1] + 1.96 * model.bse[1]\n",
    "print('95% confidence interval: [{}, {}]'.format(lower_bound, upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, it appears `statsmodels` already beat us to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(logwage, sm.add_constant(educ)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 95% confidence interval does not contain 0, and so $\\beta$ is unlikely to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we said that $\\hat{\\beta}$ has some normal distribution with mean $\\beta$ if certain assumptions are satisfied. We now can see that the standard deviation of that normal distribution is the standard error of $\\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this to test a null hypothesis that $\\beta = 0$. To do so, construct a t-statistic (which `statsmodels` does for you) that indicates how many standard deviations away $\\hat{\\beta}$ is from 0, assuming that the distribution of $\\hat{\\beta}$ is in fact centered at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(logwage, sm.add_constant(educ)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $\\hat{\\beta}$ is 74 standard deviations away from the null hypothesis mean of 0, which is an enormous number. How likely do you think it is to draw a random number roughly 74 standard deviations away from the mean, assuming a standard normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially 0. This is strong evidence that the mean of the distribution (the mean of $\\hat{\\beta}$ is the true value $\\beta$) is not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with a Binary Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary variable is a variable that takes on the value of 1 if some condition is true, and 0 otherwise. These are also called dummy variables or indicator variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might sound strange at first, but you can actually perform regression of a variable like log earnings onto a binary variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a different dataset that has the following features. Some will be useful to us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy = Table.read_table('nlsy_cleaned_small.csv')\n",
    "nlsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize log earnings vs. the binary variable corresponding to whether or not an observation went to college."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = nlsy.column('college')\n",
    "logearn = nlsy.column('log_earn_1999')\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coll, logearn)\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Log Earnings')\n",
    "plt.title('Log Earnings vs. College Completion')\n",
    "plt.xticks([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_college = nlsy.where('college', 0).column(\"log_earn_1999\")\n",
    "has_college = nlsy.where('college', 1).column(\"log_earn_1999\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Log Earnings')\n",
    "plt.title('Log Earnings vs. College Completion')\n",
    "plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)\n",
    "plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(logearn, sm.add_constant(coll)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This regression would imply that we expect, on average, observations who went to college to have 70% higher earnings than those who did not go to college. Let's now plot this line on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coll, logearn)\n",
    "plt.plot(np.linspace(0, 1), model.params[1] * np.linspace(0, 1) + model.params[0], color='tab:orange')\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Log Earnings')\n",
    "plt.title('Log Earnings vs. College Completion')\n",
    "plt.xticks([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_college = nlsy.where('college', 0).column(\"log_earn_1999\")\n",
    "has_college = nlsy.where('college', 1).column(\"log_earn_1999\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.linspace(0, 1), model.params[1] * np.linspace(0, 1) + model.params[0], color='tab:green')\n",
    "plt.xlabel('College')\n",
    "plt.ylabel('Log Earnings')\n",
    "plt.title('Log Earnings vs. College Completion')\n",
    "plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)\n",
    "plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform a simple regression onto just a dummy variable, it is an important fact that $\\hat{\\alpha}$ is the mean value of $y$ for all observations in the sample where $x = 0$, and $\\hat{\\beta}$ is the difference between the mean value of $y$ for observations in the sample where $x = 1$ and observations where $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proving this claim is beyond our scope this week, but let's verify it with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_logearn_coll = np.mean(logearn[coll == 1])\n",
    "avg_logearn_nocoll = np.mean(logearn[coll == 0])\n",
    "\n",
    "print('Avg logearn for coll = 1: ', avg_logearn_coll)\n",
    "print('Avg logearn for coll = 0: ', avg_logearn_nocoll)\n",
    "print('Difference between the two: ', avg_logearn_coll - avg_logearn_nocoll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: ', model.params[0])\n",
    "print('Slope: ', model.params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable Regression and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our procedure earlier showed that we expect to see roughly a 70% increase in earnings in people who went to college vs. people who did not go to college. Does this imply that your decision to go to college was worthwhile, and now you can expect to have roughly 70% higher earnings compared to the version of you who did not go to college?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our discussion of experiments from earlier. In an ideal experiment, we would want a good sample of people who are about to graduate high school, and then randomly assign them to either a treatment group that gets sent to college, and a control group that does not. If you are in the treatment group you *must* go to college, and if you are in the control group you *cannot* go to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the decision to go to college in this case is completely random, we can safely assume that the treatment and control groups are on average identical in attributes, apart from college attendance. We can therefore compare their log earnings in the future to see the effect of going to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this experiment is impossible to perform. We cannot randomly assign people to go to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's different between this ideal experiment and our regression from earlier? What's the issue with comparing the differences in log earnings for people in our sample who happened to go to college and those who did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sample, the treatment (went to college) and control (did not go to college) groups are not identical in every way except for college! People aren't randomly assigned college, they *choose* to go to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factors that cause someone to go to college are complex and lead to differences between people who chose to go to college and those who did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform regression on the variable \"college\", since the groups in the sample are different, not only are we capturing the effect of going to college, but we are also capturing the effects of everything else that is different about the two groups that also affects earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine another variable that captures the wealth of your family. College can be very expensive, so it might be the case that the wealthier your family is, the more likely you are to go to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it's easy to imagine that the wealthier your family is, the wealthier you are likely to be in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the group of people in the sample who went to college will tend to be wealthier than the group that did not. Also, the group of people who went to college is expected to earn more not necessarily because they went to college, but simply because they are wealthier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, when we do regression to measure the differences in earnings between people who went to college and those who did not, we also capture differences in earnings between people who grew up wealthier and those who did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, we *over-estimate* the effect of going to college. $\\hat{\\beta}$ captures the average observed benefit of going to college *and* being wealthier, but we're only interested in college. This is called *omitted variable bias*. Family wealth is an omitted variable in this regression and it's causing our results to be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think of another example of omitted variable bias. In the NLSY dataset, there is a variable called \"AFQT\". AFQT is a score on a particular standardized test that all people in the sample took."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use AFQT as a proxy measurement for the abstract idea of academic capability. While a standardized test is by no means the sole indication of someone's ability or intelligence, let's ignore that very complicated issue for now and assume that AFQT does an ok job at capturing this ability variable that is otherwise very difficult to measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there omitted variable bias from AFQT? Almost certainly. It seems fair to believe that people who choose to go to college are on average more academically-capable, and it also seems fair to say that on average we expect more capable people to earn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $\\hat{\\beta}$ above might be capturing the effects of being more capable, along with the effects of going to college."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we fix this issue? Multivariable regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only been regressing outcome variable $y$ onto one explanatory variable $x$. To find the regression line, we choose $\\hat{\\alpha}$ and $\\hat{\\beta}$ that minimize the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we believe that $y$ is actually determined by two variables, $x_1$ and $x_2$? Specifically, what if this is the \"true\" model:\n",
    "\n",
    "$$\n",
    "y = \\alpha + \\beta_1 x_{1} + \\beta_2 x_{2} + \\epsilon\n",
    "$$\n",
    "\n",
    "and we would like to estimate the following:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}_1 x_{1} + \\hat{\\beta}_2 x_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our challenge is choosing $\\hat{\\alpha}$, $\\hat{\\beta}_1$, *and* $\\hat{\\beta}_2$ that minimize the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(target, pred):\n",
    "    return np.sqrt(mean_squared_error(target, pred)) \n",
    "\n",
    "def to_minimize(intercept, beta_1, beta_2):\n",
    "    predictions = intercept + beta_1 * nlsy.column('college') + beta_2 * nlsy.column(\"AFQT\")\n",
    "    actual = nlsy.column('log_earn_1999')\n",
    "    return rmse(predictions, actual)\n",
    "\n",
    "minimize(to_minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do multivariable regression in Python using the `statsmodels` package, the procedure is largely the same with some minor differences. Instead of only selecting 1 column for $x$, we need to specify that $x$ is in fact multiple variables by using `select` to select out the relevant columns. Note that here, we take out the values of each by adding `.values` to the end. This is because `statsmodels` only knows how to work with NumPy arrays, not tables.\n",
    "\n",
    "```python\n",
    "X = tbl.select(features).values             # Separate features (explanatory and control variables) \n",
    "y = tbl.column(target)                      # Separate outcome variable\n",
    "model = sm.OLS(y, sm.add_constant(X))       # Initialize the OLS regression model\n",
    "result = model.fit()                        # Fit the regression model and save it to a variable\n",
    "result.summary()                            # Display a summary of results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nlsy.column('log_earn_1999')\n",
    "X = nlsy.select('college', 'AFQT').values.astype(float)\n",
    "\n",
    "model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\hat{\\beta}_1$ is 0.43, compared to 0.72 from the earlier \"biased\" regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's huge! This implies that when we control for a person's ability (i.e. we get rid of that source of bias), we only see that on average going to college is associated with a 43% increase in earnings instead of 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, looking at the 95% confidence interval for $\\hat{\\beta}_2$, we see that it does not contain 0, which would imply that AFQT score has a strong non-zero association with earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These observations validate our claim that AFQT was probably an omitted variable causing $\\hat{\\beta}_1$ to be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting $\\hat{\\beta}$ coefficients, you should always be mindful of potential sources of bias that could make your coefficients misleading and not useful from an econometric context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multivariable Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\n",
    "ax.scatter(nlsy.column(\"AFQT\"), \n",
    "           nlsy.column(\"college\"), \n",
    "           nlsy.column('log_earn_1999'))\n",
    "plt.xlabel(\"AFQT\")\n",
    "plt.ylabel(\"college\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid(np.arange(0,100,1), np.arange(0,1,0.01))\n",
    "Z = 0.0084 * X + 0.4301 * Y + 9.9563\n",
    "ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\n",
    "ax.scatter(nlsy.column(\"AFQT\"), \n",
    "           nlsy.column(\"college\"), \n",
    "           nlsy.column('log_earn_1999'))\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",
    "plt.xlabel(\"AFQT\")\n",
    "plt.ylabel(\"college\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid(np.arange(0,1,0.01), np.arange(0,100,1))\n",
    "Z = 0.4301 * X + 0.0084 * Y + 9.9563\n",
    "ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\n",
    "ax.scatter(nlsy.column(\"college\"), \n",
    "           nlsy.column(\"AFQT\"), \n",
    "           nlsy.column('log_earn_1999'))\n",
    "ax.plot_surface(X, Y, Z, alpha=0.1)\n",
    "plt.ylabel(\"AFQT\")\n",
    "plt.xlabel(\"college\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colinearity and Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do regression onto the variable \"college\", why don't we also include a variable that measures not going to college? In other words, why don't we regress on college and the opposite of college so that way we can get an estimate of the average earnings of college-goers and non-college-goers. Why do we do this roundabout way of using the intercept term and a difference in means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a dataset with a variable for college, a variable for not going to college, and the intercept term. The issue with this is that there is now redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at just one element of the sample. Let's say this person went to college, so this person's features are the following:\n",
    "* College = 1\n",
    "* Not College = 0\n",
    "* Intercept term = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is a redundancy here; you can guess one of the variables from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, by redundancy we mean that *one variable can be written as a linear combination of the other variables*. In fact, there are three different combinations:\n",
    "* Intercept = College + Not College\n",
    "* Not College = Intercept - College\n",
    "* College = Intercept - Not College"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equalities aren't just true for this one person; they actually hold true for any possible person in the sample. This is because of the way we defined \"college\" and \"not college\". You can't simultaneously be in both, and so adding them together you get 1, which is just the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we have redundancy whenever we have *mutually exclusive* and *exhaustive* dummy variables in combination with an intercept term.\n",
    "* Mutually exclusive: You cannot be in more than one dummy variable.\n",
    "* Exhaustive: You must be in at least one dummy variable.\n",
    "\n",
    "You can see that \"college\" and \"not college\" satisfy these conditions. So why is this redundancy an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes ambiguous what the values for $\\hat{\\alpha}$, $\\hat{\\beta}_1$, and $\\hat{\\beta}_2$ should be in the model where we include all three terms:\n",
    "\n",
    "$$\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\text{college} + \\hat{\\beta}_2 \\text{not college}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a case where we expect people who went to college to have log earnings of 10 and those who did not go to college to have log earnings of 8. What values for $\\hat{\\beta}$ and $\\hat{\\alpha}$ make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\hat{\\beta}_1 = 10$\n",
    "* $\\hat{\\beta}_2 = 8$\n",
    "* $\\hat{\\alpha} = 0$\n",
    "\n",
    "make sense. These are valid values for $\\hat{\\beta}$ and $\\hat{\\alpha}$ that satisfy the condition above. To see why, consider a person with college:\n",
    "\n",
    "$$\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\cdot 1 + \\hat{\\beta}_2 \\cdot 0 \\\\\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\\\\n",
    "\\text{log earnings} = 0 + 10 = 10\n",
    "$$\n",
    "\n",
    "and a person without college:\n",
    "\n",
    "$$\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\cdot 0 + \\hat{\\beta}_2 \\cdot 1 \\\\\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_2 \\\\\n",
    "\\text{log earnings} = 0 + 8 = 8\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\hat{\\beta}_1 = 2$\n",
    "* $\\hat{\\beta}_2 = 0$\n",
    "* $\\hat{\\alpha} = 8$\n",
    "\n",
    "also make sense. To see why, consider a person with college:\n",
    "\n",
    "$$\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\cdot 1 + \\hat{\\beta}_2 \\cdot 0 \\\\\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\\\\n",
    "\\text{log earnings} = 8 + 2 = 10\n",
    "$$\n",
    "\n",
    "and a person without college:\n",
    "\n",
    "$$\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_1 \\cdot 0 + \\hat{\\beta}_2 \\cdot 1 \\\\\n",
    "\\text{log earnings} = \\hat{\\alpha} + \\hat{\\beta}_2 \\\\\n",
    "\\text{log earnings} = 8 + 0 = 8\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, there are actually infinitely many solutions for $\\hat{\\beta}$ that satisfy the condition where people who went to college have mean log earnings of 10 and people who did not go to college have mean log earnings of 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This holds true for all situations where you regress on a constant and a set of mutually exclusive and exhaustive dummies. There is no unique solution for $\\hat{\\beta}$, which is a problem for econometricians who want unique and interpretable coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there is mathematical justification for this as well. At some point in the math involved in performing regression, having redundant variables causes a division by 0. This is particularly upsetting for your computer, and it will complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we avoid this problem? We deliberately exclude one of the variables. It can technically either be one of the dummy variables or the intercept term, but we usually really want to have an intercept term present in our regression for other reasons. So we usually get rid of one of the dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we implicitly did this earlier. We did not include \"not college\" in our first regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
